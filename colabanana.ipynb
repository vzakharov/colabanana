{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colabanana ‚Äî test your üçå Banana environment with Colab\n",
    "\n",
    "## What is this?\n",
    "\n",
    "This is a simple notebook to test your [Banana serverless](https://docs.banana.dev/banana-docs/core-concepts/inference-server/serverless-framework) environments right from your browser. It is assumed that you already know the core concepts of Banana and have downloaded/cloned the [Colabanana repo](https://github.com/vzakharov/colabanana) from GitHub.\n",
    "\n",
    "For more information on Banana and its serverless framework, please refer to the [Banana docs](https://docs.banana.dev/banana-docs/).\n",
    "\n",
    "For information on why this notebook exists, please refer to the [Colabanana repo](https://github.com/vzakharov/colabanana).\n",
    "\n",
    "## How to use this notebook?\n",
    "\n",
    "1. Open this notebook in Colab. (Funny because you are already here but hey, you never know.)\n",
    "\n",
    "2. Go one by one through the cells and either edit them as suggested or enter the required information in the forms attached to them. I tried giving as much information as needed in each cell, so I won‚Äôt repeat it here.\n",
    "\n",
    "3. Once done, go `Runtime` > `Run all` and watch the magic happen.\n",
    "\n",
    "The notebook is already pre-populated to use the HuggingFace [fill-mask](https://huggingface.co/tasks/fill-mask) model, so you can test it right away.\n",
    "\n",
    "Note that working with the notebook will involve a lot of copying and pasting between your `.py` files and respective cells in the notebook, but that‚Äôs the price you pay for the convenience of using Colab. (I strongly suggest that you do the editing in your `.py` files and then copy-paste the code into the notebook, not the other way around.)\n",
    "\n",
    "Once everything works perfectly, you can proceed to deploy your model to a serverless environment according to the [Banana docs](https://docs.banana.dev/banana-docs/core-concepts/inference-server/serverless-framework). You shouldn‚Äôt need any changes to your `.py` files for that. Well, at least *theoretically*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title requirements.txt\n",
    "## üëá Copy the contents of `requirements.txt` between \"\"\" and \"\"\" in the code below\n",
    "\n",
    "requirements_txt = \"\"\"\n",
    "# Basic, needed for the server to run\n",
    "flask\n",
    "\n",
    "# Model-specific\n",
    "transformers\n",
    "accelerate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Initial setup\n",
    "#üößüößüöß Ignore this cell; it's just some service code to make the notebook work\n",
    "\n",
    "try:\n",
    "  packages_installed\n",
    "except NameError:\n",
    "  packages_installed = []\n",
    "\n",
    "def pip_install(package):\n",
    "  \n",
    "  global packages_installed\n",
    "\n",
    "  if package not in packages_installed:\n",
    "    print(f\"Installing {package}\")\n",
    "    !pip install {package}\n",
    "    packages_installed += [ package ]\n",
    "  else:\n",
    "    print(f\"{package} already installed\")\n",
    "\n",
    "for package in requirements_txt.split(\"\\n\"):\n",
    "  if package and not package.startswith(\"#\"):\n",
    "    pip_install(package)\n",
    "\n",
    "pip_install(\"pyngrok\")\n",
    "ngrok_token = \"\" #@param {type:\"string\"}\n",
    "#@markdown We need [ngrok](https://ngrok.com) to make the server accessible from the outside world via a public URL. If you don‚Äôt have an ngrok token, you can get one [here](https://dashboard.ngrok.com/get-started/your-authtoken). Ngrok‚Äôs free tier is enough for [most testing purposes](https://ngrok.com/pricing).\n",
    "\n",
    "assert ngrok_token != \"\", \"You need to enter your ngrok token in the cell above (see https://gyazo.com/c6f0aaf59cd6aee912da7357f2f736db)\"\n",
    "\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# (Uncomment the above line if you want to use Google Drive to store the model weights)\n",
    "# (Do NOT comment out `from google.colab import drive` because we use this as a flag to determine whether we are running on Colab or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download the model\n",
    "#@markdown ## üëà Copy the contents of `download.py` here\n",
    "#@markdown **Note:** Ideally, you want to rewrite the models so that they are downloaded and later reused from Google Drive. This way, you won't need to download the model every time you start a runtime. However, the code will be specific to the model you are using, so we will leave it as an exercise for the notebook user.\n",
    "\n",
    "# In this file, we define download_model_weights\n",
    "# It runs during container build time to get model weights built into the container\n",
    "\n",
    "# In this example: A Huggingface BERT model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def download_model_weights():\n",
    "\n",
    "  # üößüößüöß Service code to avoid downloading the model every time the cell is run in Colab; ignore until the next üößüößüöß\n",
    "  try:\n",
    "\n",
    "    weights_downloaded\n",
    "    # Hereinafter, this trick allows us to avoid downloading the model whenever the cell is run. Once the model is downloaded for the first time, the variable `weights_downloaded` is set to True, so no error is raised and the model (which is downloaded in the except block) is not downloaded again.\n",
    "\n",
    "  except NameError:\n",
    "  # üõ£Ô∏èüõ£Ô∏èüõ£Ô∏è End of service code; proceed with your code below\n",
    "  \n",
    "    \n",
    "    # do a dry run of loading the huggingface model, which will download weights\n",
    "    pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "\n",
    "  # üößüößüöß Ignore all code below this line\n",
    "    weights_downloaded = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Init is ran on server startup\n",
    "# Load your model to GPU as a global variable here using the variable name \"model\"\n",
    "def init():\n",
    "  global model\n",
    "  \n",
    "  device = 0 if torch.cuda.is_available() else -1\n",
    "  model = pipeline('fill-mask', model='bert-base-uncased', device=device)\n",
    "\n",
    "# Inference is ran for every server call\n",
    "# Reference your preloaded global model variable here.\n",
    "def inference(model_inputs:dict) -> dict:\n",
    "  global model\n",
    "\n",
    "  # Parse out your arguments\n",
    "  prompt = model_inputs.get('prompt', None)\n",
    "  if prompt == None:\n",
    "    return {'message': \"No prompt provided\"}\n",
    "  \n",
    "  # Run the model\n",
    "  result = model(prompt)\n",
    "\n",
    "  # Return the results as a dictionary\n",
    "  return result\n",
    "\n",
    "#@title Define model init/inference functions\n",
    "#@markdown ## üëà Copy the contents of `app.py` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Define the test inputs\n",
    "#@markdown ## üëà Copy the contents of `test.py` here\n",
    "\n",
    "# You generally only need to change the `default_model_inputs` variable, which defines the default inputs to your model\n",
    "\n",
    "default_model_inputs = dict(\n",
    "\n",
    "  prompt = \"Hello I am a [MASK] model.\"\n",
    "\n",
    ")\n",
    "\n",
    "# üößüößüöß Do not modify the code below this line unless you're sure you know what you're doing üößüößüöß\n",
    "\n",
    "#@markdown **Notes:**\n",
    "#@markdown There are two ways to run the test:\n",
    "#@markdown - Using the /test endpoint after the server is running (see the last cell of this notebook), or\n",
    "#@markdown - By running `python3 test.py` in the terminal (from anywhere, e.g. your local machine).\n",
    "#@markdown\n",
    "#@markdown In the case of the /test endpoint, you can provide the model inputs as query parameters (e.g. `http://<some_id>.ngrok.io/test?prompt=Hello%20I%20am%20a%20[MASK]%20model.`). In the case of the terminal, you can provide the model inputs as command line arguments (e.g. `python3 test.py --prompt \"Hello I am a [MASK] model.\"`).\n",
    "#@markdown\n",
    "#@markdown #### Testing environment\n",
    "#@markdown When running as a script (`python3 test.py`), you can specify which environment to use (dev/prod) by adding the `--env` argument, e.g. `python3 test.py --env prod`. If you don't specify the environment, you will be prompted to enter it or skip it (defaulting to `dev`):\n",
    "#@markdown - Choosing `prod` will make a call to an already deployed Banana server;\n",
    "#@markdown - Choosing `dev` will make a call to the server running in the current notebook.\n",
    "#@markdown\n",
    "#@markdown In the case of `dev`, you will need to provide the public URL of the server, which you can find in the console after running the last cell of this notebook). We do not store this information locally on purpose, as the URL is likely to change every time you run the notebook.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def test_inference(model_inputs={}):\n",
    "\n",
    "  meta_args = {}\n",
    "  meta_keys = ['env', 'api_key', 'model_key']\n",
    "\n",
    "  # If no model_inputs are provided, either use the ones from command line (if any) or the default ones\n",
    "  if model_inputs == {}:\n",
    "    # Check if any command line arguments were provided. Keep in mind that the command line arguments is of form \"python3 test.py arg1 arg2 arg3\"\n",
    "    if not 'google.colab' in sys.modules and len(sys.argv) > 1:\n",
    "\n",
    "      print(f\"Using command line arguments: {sys.argv[1:]}\")\n",
    "\n",
    "      # The inputs would be provided in the form --[json key] [json value]\n",
    "      # For example: --prompt \"Hello I am a [MASK] model.\"\n",
    "      # For meta keys, add them to the meta_args dict instead\n",
    "      for i in range(1, len(sys.argv), 2):\n",
    "        key = sys.argv[i].replace('--', '')\n",
    "        value = sys.argv[i+1]\n",
    "        dict_to_use = model_inputs if key not in meta_keys else meta_args\n",
    "        dict_to_use[key] = value\n",
    "\n",
    "    else:\n",
    "      # Default inputs\n",
    "      model_inputs = default_model_inputs\n",
    "  \n",
    "  print(f\"Using model inputs: {model_inputs}\")\n",
    "  print(f\"Using meta args: {meta_args}\")\n",
    "\n",
    "  if 'google.colab' in sys.modules:\n",
    "\n",
    "    url = ngrok_tunnel.public_url\n",
    "    res = requests.post(url, json=model_inputs)\n",
    "\n",
    "  else:\n",
    "\n",
    "    # Check which environment we're in (dev/prod)\n",
    "    env = meta_args['env'] if 'env' in meta_args else input(\"Enter environment (dev/prod) or press Enter to use dev: \") or 'dev'\n",
    "\n",
    "    if env == 'prod':\n",
    "\n",
    "      credential_prompts = dict(\n",
    "        api_key = \"Enter your API key (go to https://app.banana.dev/ to get one)\",\n",
    "        model_key = \"Enter your model key (from your model's page on https://app.banana.dev/)\"\n",
    "      )\n",
    "\n",
    "      # Load credentials from 'credentials.json' if it exists\n",
    "      if os.path.exists('credentials.json'):\n",
    "        with open('credentials.json', 'r') as f:\n",
    "          credentials = json.loads(f.read())\n",
    "          print(\"Loaded credentials from credentials.json\")\n",
    "      else:\n",
    "        credentials = {}\n",
    "\n",
    "      credentials_changed = False\n",
    "      for key, prompt in credential_prompts.items():\n",
    "        if key not in meta_args:\n",
    "          if key in credentials:\n",
    "            print(f\"Using {key} from credentials.json\")\n",
    "          else:\n",
    "            credentials[key] = input(f\"{prompt}: \")\n",
    "            credentials_changed = True\n",
    "        else:\n",
    "          credentials[key] = meta_args[key]\n",
    "          credentials_changed = True\n",
    "      \n",
    "      # Save credentials to 'credentials.json' if they changed\n",
    "      if credentials_changed:\n",
    "        with open('credentials.json', 'w') as f:\n",
    "          f.write(json.dumps(credentials))\n",
    "          print(\"Saved credentials to credentials.json\")\n",
    "\n",
    "      res = requests.post(\"https://api.banana.dev/start/v4/\", json=dict(\n",
    "        apiKey = credentials['api_key'],\n",
    "        modelKey = credentials['model_key'],\n",
    "        modelInputs = model_inputs\n",
    "      )).json()\n",
    "\n",
    "    else:\n",
    "\n",
    "      url = input(\"Enter the public URL of your server (e.g. http://<some_id>.ngrok.io, look for it in the console after running the notebook's last cell): \")\n",
    "      res = requests.post(url, json=model_inputs)\n",
    "\n",
    "  print(f\"Response: {res}\")\n",
    "\n",
    "  return res\n",
    "\n",
    "# If not running in Colab, run the test\n",
    "if not 'google.colab' in sys.modules:\n",
    "  test_inference()\n",
    "else:\n",
    "  print(\"Running in Colab, skipping test (you will be able to run it later by using the /test endpoint)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Start the server\n",
    "#@markdown Usually, you don‚Äôt need to change anything in this cell. However, if you change the `server.py` file, double-click on the cell to edit the code and paste its  contents.\n",
    "\n",
    "import asyncio\n",
    "import datetime\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "\n",
    "try:\n",
    "  port += 1\n",
    "except NameError:\n",
    "  port = 8000\n",
    "  # (This is a hack to avoid \"address already in use\" errors when running the cell multiple times)\n",
    "\n",
    "if not 'google.colab' in sys.modules:\n",
    "\n",
    "  import app as user_src\n",
    "  user_src.init()\n",
    "\n",
    "else:\n",
    "\n",
    "  try:\n",
    "    user_src\n",
    "    print(\"Model already initialized\")\n",
    "  except NameError:\n",
    "    # Define a user_src object which has attributes for init and inference\n",
    "    class user_src:\n",
    "      init = init\n",
    "      inference = inference\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    user_src.init()\n",
    "\n",
    "  # Start the ngrok tunnel\n",
    "  print(\"Starting tunnel\")\n",
    "\n",
    "  for tunnel in ngrok.get_tunnels():\n",
    "    ngrok.disconnect(tunnel.public_url)      \n",
    "  # (We need this to remove the already created tunnel if running the cell multiple times, as free ngrok only allows so many tunnels)\n",
    "\n",
    "  ngrok_tunnel = ngrok.connect(port)\n",
    "  print(ngrok_tunnel)\n",
    "  print(\"This is the public URL of your server ‚òùÔ∏è‚òùÔ∏è‚òùÔ∏è (you can also use https)\")\n",
    "  # The public URL will be printed to the console after this line, so look for it there\n",
    "\n",
    "# We do the model load-to-GPU step on server startup\n",
    "# so the model object is available globally for reuse\n",
    "\n",
    "# Create the http server app.\n",
    "try:\n",
    "  # First let's kill the server and its server_thread if it's already running (on Colab)\n",
    "  del server\n",
    "  del server_thread\n",
    "  print(\"Stopped running server\")\n",
    "except ( AssertionError, NameError ):\n",
    "  pass\n",
    "  \n",
    "# Use timestamp in server name for debug purposes\n",
    "server = Flask(f\"server-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\")\n",
    "\n",
    "# Healthchecks verify that the environment is correct on Banana Serverless\n",
    "@server.route('/healthcheck', methods=['GET'])\n",
    "def healthcheck():\n",
    "  # dependency free way to check if GPU is visible\n",
    "  gpu = False\n",
    "  out = subprocess.run(\"nvidia-smi\", shell=True)\n",
    "  if out.returncode == 0: # success state on shell command\n",
    "    gpu = True\n",
    "  return jsonify(dict(\n",
    "    state=\"healthy\",\n",
    "    gpu=gpu,\n",
    "    timestamp=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "  ))\n",
    "\n",
    "# Inference POST handler at '/' is called for every http call from Banana\n",
    "@server.route('/', methods=['POST'])\n",
    "def inference():\n",
    "  model_inputs = request.get_json()\n",
    "  print(f\"Received request: {model_inputs}\")\n",
    "\n",
    "  output = user_src.inference(model_inputs)\n",
    "  print(f\"Sending response: {output}\")\n",
    "\n",
    "  # If the output is an ndarray, convert it to a list (of lists, of lists, etc.)\n",
    "  if isinstance(output, np.ndarray):\n",
    "    # Go recursively through each dimension and convert to list\n",
    "    def convert_to_list(x):\n",
    "      if isinstance(x, np.ndarray):\n",
    "        return [convert_to_list(y) for y in x]\n",
    "      else:\n",
    "        return float(x)\n",
    "\n",
    "    output = convert_to_list(output)\n",
    "\n",
    "  return jsonify(output)\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "  # GET /test to call a sample inference using the public URL from ngrok_tunnel\n",
    "  @server.route('/test', methods=['GET'])\n",
    "  def test_endpoint():\n",
    "\n",
    "\n",
    "    global test_inference\n",
    "\n",
    "    url = ngrok_tunnel.public_url\n",
    "    print(f\"Sending a test inference request to {url}\")\n",
    "\n",
    "    # Take model_inputs from query params\n",
    "    model_inputs = request.args.to_dict()\n",
    "\n",
    "    print(f\"Request: {model_inputs}\")\n",
    "    print(\"‚úÇ=== Below are logs from the server processing the test request\\n\")\n",
    "\n",
    "    res = test_inference(model_inputs)\n",
    "\n",
    "    print(\"\\n‚úÇ=== End of logs from the server processing the test request\")\n",
    "    print(f\"Response: {res.json()}\")\n",
    "    return jsonify(res.json())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Start the  server in a new thread\n",
    "  print(\"Starting server\")\n",
    "  import threading\n",
    "  server_thread = threading.Thread(target=server.run, kwargs={\"host\": \"0.0.0.0\", \"port\": port})\n",
    "  server_thread.start()\n",
    "\n",
    "  if 'google.colab' in sys.modules:\n",
    "    # Print that the server is started and a test URL after a second (so the server has time to start)\n",
    "    time.sleep(1)\n",
    "    print(f\"Server started; test: {ngrok_tunnel.public_url}/test\")\n",
    "\n",
    "    # Keep the cell running so we can see the logs\n",
    "    print(\"Keeping the cell running so you can see the server logs\")\n",
    "    while True:\n",
    "      time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That‚Äôs it!\n",
    "### Now test your model by either running `python test.py` in your *local* terminal or using the `/test` endpoint in Colab (see the logs in the cell above). Good luck!\n",
    "\n",
    "For more information on building with and deploying Banana‚Äôs serverless framework, see https://docs.banana.dev/banana-docs/core-concepts/inference-server/serverless-framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
