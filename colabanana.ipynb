{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title pip-installs & stuff\n",
    "#@markdown ## Install the required packages\n",
    "\n",
    "!pip install sanic\n",
    "# We use Sanic to create the http server\n",
    "\n",
    "!pip install pyngrok\n",
    "ngrok_token = \"2yzWKXfe8GNcHdJWUxige_2YBbXvueHEYW17jM2E7hY\" #@param {type:\"string\"}\n",
    "#@markdown We need [ngrok](https://ngrok.com) to make the server accessible from the outside world via a public URL. If you donâ€™t have an ngrok token, you can get one [here](https://dashboard.ngrok.com/tunnels/authtokens). Ngrokâ€™s free tier is enough for [most testing purposes](https://ngrok.com/pricing).\n",
    "\n",
    "# The rest of the packages are optional and depend on the model you are using\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# (Uncomment the above line if you want to use Google Drive to store the model weights)\n",
    "\n",
    "#@ ### You can run the Notebook now. Click `Runtime` -> `Run all` in the menu above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download the model\n",
    "#@markdown ## ðŸ‘ˆ Copy the contents of `download.py` here\n",
    "#@markdown **Note:** Ideally, you want to rewrite the models so that they are downloaded and later reused from Google Drive. This way, you won't need to download the model every time you start a runtime. However, the code will be specific to the model you are using, so we will leave it as an exercise for the notebook user.\n",
    "\n",
    "# In this file, we define download_model_weights\n",
    "# It runs during container build time to get model weights built into the container\n",
    "\n",
    "# In this example: A Huggingface BERT model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def download_model_weights():\n",
    "\n",
    "  try:\n",
    "\n",
    "    weights_downloaded\n",
    "    # Hereinafter, this trick allows us to avoid downloading the model whenever the cell is run. Once the model is downloaded for the first time, the variable `weights_downloaded` is set to True, so no error is raised and the model (which is downloaded in the except block) is not downloaded again.\n",
    "\n",
    "  except NameError:\n",
    "    \n",
    "    # do a dry run of loading the huggingface model, which will download weights\n",
    "    pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "    weights_downloaded = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Define model init/inference functions\n",
    "#@markdown ## ðŸ‘ˆ Copy the contents of `app.py` here\n",
    " \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Init is ran on server startup\n",
    "# Load your model to GPU as a global variable here using the variable name \"model\"\n",
    "def init():\n",
    "    global model\n",
    "\n",
    "    try:\n",
    "      model\n",
    "      print(\"Model already loaded\")\n",
    "    except NameError:\n",
    "    \n",
    "      device = 0 if torch.cuda.is_available() else -1\n",
    "      model = pipeline('fill-mask', model='bert-base-uncased', device=device)\n",
    "      \n",
    "      print(\"Model loaded\")\n",
    "\n",
    "# Inference is ran for every server call\n",
    "# Reference your preloaded global model variable here.\n",
    "def inference(model_inputs:dict) -> dict:\n",
    "    global model\n",
    "\n",
    "    # Parse out your arguments\n",
    "    prompt = model_inputs.get('prompt', None)\n",
    "    if prompt == None:\n",
    "        return {'message': \"No prompt provided\"}\n",
    "    \n",
    "    # Run the model\n",
    "    result = model(prompt)\n",
    "\n",
    "    # Return the results as a dictionary\n",
    "    return result\n",
    "\n",
    "# If testing with Colab, define a user_src object which has attributes for init and inference\n",
    "# Check if imported modules include google.colab\n",
    "if 'google.colab' in sys.modules:\n",
    "  class UserSrc:\n",
    "    def __init__(self):\n",
    "      self.init = init\n",
    "      self.inference = inference\n",
    "\n",
    "  user_src = UserSrc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Start the server\n",
    "#@markdown ## ðŸ‘ˆ Copy the contents of `server.py` here\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from sanic import Sanic, response\n",
    "\n",
    "try:\n",
    "  port += 1\n",
    "except NameError:\n",
    "  port = 8000\n",
    "  # (This is a hack to avoid \"address already in use\" errors when running the cell multiple times)\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "  Sanic._app_registry = {}\n",
    "  # (We need this to remove the already created app if running the cell multiple times)\n",
    "\n",
    "  # Start the ngrok tunnel\n",
    "\n",
    "  try:\n",
    "    ngrok_tunnel\n",
    "    print(\"Tunnel already started\")\n",
    "    print(\"To delete the tunnel, run:\\n\\nngrok.disconnect(ngrok_tunnel)\\ndel ngrok_tunnel\")\n",
    "  except NameError:\n",
    "\n",
    "    from pyngrok import ngrok\n",
    "\n",
    "    ngrok_tunnel = ngrok.connect(port)\n",
    "    # The public URL will be printed to the console after this line, so look for it there\n",
    "    # It will look like this:\n",
    "    # <NgrokTunnel: \"http://<some_id>.ngrok.io\" -> \"http://localhost:8000\">\n",
    "\n",
    "else:\n",
    "\n",
    "  import app as user_src\n",
    "  # (If testing with colab, the app interface is defined in the cell above)\n",
    "\n",
    "# We do the model load-to-GPU step on server startup\n",
    "# so the model object is available globally for reuse\n",
    "user_src.init()\n",
    "\n",
    "# Create the http server app.\n",
    "\n",
    "\n",
    "server = Sanic(\"my_app\")\n",
    "\n",
    "# Healthchecks verify that the environment is correct on Banana Serverless\n",
    "@server.route('/healthcheck', methods=[\"GET\"])\n",
    "def healthcheck(request):\n",
    "  # dependency free way to check if GPU is visible\n",
    "  gpu = False\n",
    "  out = subprocess.run(\"nvidia-smi\", shell=True)\n",
    "  if out.returncode == 0: # success state on shell command\n",
    "    gpu = True\n",
    "\n",
    "  return response.json({\"state\": \"healthy\", \"gpu\": gpu})\n",
    "\n",
    "# Inference POST handler at '/' is called for every http call from Banana\n",
    "@server.route('/', methods=[\"POST\"]) \n",
    "def inference(request):\n",
    "  try:\n",
    "    model_inputs = response.json.loads(request.json)\n",
    "  except:\n",
    "    model_inputs = request.json\n",
    "\n",
    "  output = user_src.inference(model_inputs)\n",
    "\n",
    "  return response.json(output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  server.run(host='0.0.0.0', port=port, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Thatâ€™s it!\n",
    "#@markdown ### Now test your model by tunning `python test.py` in your *local* terminal. You will be prompted for the public URL of your server, which you can find in the console after running the cell above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
